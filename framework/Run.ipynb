{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Example of training SimCLR18 is here](https://b1g7e2p8fugfc16727hk.storage.yandexcloud.net/74cd94c0-a86e-49b9-9eee-487b9e0a14b9/user-data/resources/system/ff5260c4-b03f-4d1e-8a7a-0a7b154c4def0ae2b15e-b5eb-4626-976e-c22f5104ffa4.html?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200603T101809Z&X-Amz-SignedHeaders=host&X-Amz-Expires=604800&X-Amz-Credential=mXap0Jx_lcNqHbMis1m-%2F20200603%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=6434bb77a8af469c9ab8fedf107a47568004b29b3008068a17fcf9edc5af03e9)\n",
    "[Model checkpoints are here](https://drive.google.com/drive/folders/1zuOWRdT2QX4UQtDaS23XcrjAq1SZra4I?usp=sharing)\n",
    "\n",
    "[Converted weights of pretrained Resnet50-1x from the original article are here](https://drive.google.com/file/d/1YgouQx4Vn2st--GwO7r2MGTs-fiUWCKP/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch import cuda\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython import display\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from pathlib import Path\n",
    "\n",
    "# Import scripts\n",
    "from CustomModels import CustomSimCLR50, CustomSimCLR18\n",
    "from data import DataWrapper, FeatureExtractor, CustomDataset\n",
    "from loss import ContrastiveLoss\n",
    "from aux import count_parameters, save_ckp, load_ckp, get_lr\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LATENT_DIM = 64 # as in the original article RL\n",
    "HEAD_DIM = 256  # dimensionality of heads output \n",
    "BACKBONE = 'Resnet18' # whether finetune 'Resnet50' or train/finetune 'Resnet18'\n",
    "BATCH_SIZE = 512 # try to increase \n",
    "N_EPOCHS = 100 # adjust further\n",
    "CLOUD = True # if in cloud TB doesn't work\n",
    "\n",
    "print(cuda.get_device_name())\n",
    "print(cuda.get_device_capability())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(\"data_full_v1/warehouse_time_step_0.jpg\").is_file():\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile('./data_full_v1.zip', 'r') as archive:\n",
    "        archive.extractall()\n",
    "else:\n",
    "    print('Already there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = np.asarray(PIL.Image.open('data_full_v1/warehouse_time_step_150.jpg'))\n",
    "plt.imshow(im)\n",
    "print(im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = DataWrapper(BATCH_SIZE, 0.1, (128, 128, 3), './data_full_v1')\n",
    "train_iterator, val_iterator = wrapper.get_loaders()\n",
    "print(len(train_iterator))\n",
    "print(len(val_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_iterator))\n",
    "print(len(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = transforms.ToPILImage()(sample[0][12]).convert(\"RGB\")\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "image = transforms.ToPILImage()(sample[1][12]).convert(\"RGB\")\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BACKBONE == 'Resnet50':\n",
    "    model = CustomSimCLR50(BATCH_SIZE, LATENT_DIM, HEAD_DIM).to(DEVICE)\n",
    "elif BACKBONE == 'Resnet18':\n",
    "    model = CustomSimCLR18(BATCH_SIZE, LATENT_DIM, HEAD_DIM, pretrained = True).to(DEVICE)\n",
    "else:\n",
    "    print(BACKBONE, \" doesn't match any\")\n",
    "\n",
    "criterion = ContrastiveLoss(True, 0.3, DEVICE, BATCH_SIZE) \n",
    "print(\"Current model is:\",  model.__class__.__name__)\n",
    "print(f\"Model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cuda.memory_summary(device = DEVICE, abbreviated = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, DEVICE, 't0.3HD256BS512', lr = 0.0005, weight_decay = 1e-5, gamma = 0.5, step_size = 20, \n",
    "      n_epochs = 100, cloud = CLOUD, train_iterator = train_iterator, val_iterator = val_iterator, criterion = criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_test = CustomDataset('./data_full_v1', transform = transforms.Compose([transforms.ToTensor()]))\n",
    "print(len(dataset_to_test))\n",
    "print(dataset_to_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = DataLoader(dataset_to_test, batch_size = 512, num_workers = 4, pin_memory = True, drop_last = False)\n",
    "next(iter(data_iterator)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Extractor = FeatureExtractor(model, DEVICE, data_iterator, \"./t0.3HD256BS512best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = Extractor.get_features()\n",
    "print(feature_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "principalComponents = pca.fit_transform(feature_matrix)\n",
    "print(principalComponents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 15))\n",
    "plt.scatter(principalComponents[:, 0], principalComponents[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
